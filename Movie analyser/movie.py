# -*- coding: utf-8 -*-
"""Movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T3GtV0ElqI1nIwYGH5UQa1w1_UzCMerk
"""

pip install nltk textblob  bs4 re

import pandas as pd

## loading the dataset

from google.colab import files
upload=files.upload()

df=pd.read_csv('IMDB Dataset.csv')
df.head(3)

df['sentiment'].value_counts()

import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

## Preprocess the text
### Lowercase the text
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
def clean_text(text):
    # 1. Remove HTML tags
    text = BeautifulSoup(text, "html.parser").get_text()

    # 2. Lowercase
    text = text.lower()

    # 3. Remove special characters and numbers
    text= re.sub('\[[^]]*\]', '',text)
    text = re.sub(r'[^a-z\s]', '', text)

    # 4. Tokenize
    words = nltk.word_tokenize(text)

    # 5. Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # 6. Lemmatizing (you can also use stemming instead)
    lemmatizer = WordNetLemmatizer()

    words = [lemmatizer.lemmatize(word) for word in words]

    # Final cleaned text
    return " ".join(words)

df['process_txt']=df['review'].apply(clean_text)

df.head(5)

# def get_sentiment(score):
#     if score > 0.1:
#         return 'Positive'
#     elif score < -0.1:
#         return 'Negative'
#     else:
#         return 'Neutral'

# def sentiment(data):
#     data['sentiment_score']=data['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)
#     data['sentiment']=data['sentiment_score'].apply( lambda x: get_sentiment(x))
#     return  data.drop('sentiment_score',axis=1)

# labelling the categorical value
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df['label']=encoder.fit_transform(df['sentiment'])

df.head(5)

# Handling the imbalance dataset
# from sklearn.utils import resample

# # Combine into a single DataFrame
# positive = df_2[df_2.sentiment == 'Positive']
# neutral = df_2[df_2.sentiment == 'Neutral']
# negative = df_2[df_2.sentiment == 'Negative']

# # Downsample majority classes to match negative class
# positive_down = resample(positive,
#                          replace=False,     # sample without replacement
#                          n_samples=len(negative),
#                          random_state=42)

# neutral_down = resample(neutral,
#                         replace=False,
#                         n_samples=len(negative),
#                         random_state=42)

# # Combine balanced dataset
# df_balanced = pd.concat([positive_down, neutral_down, negative])

X=df['process_txt']
Y=df['label']

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=5000)
X_tfidf = vectorizer.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.2, random_state=40)

from sklearn.linear_model import LogisticRegression

# model = LogisticRegression(max_iter=1000)
# model.fit(X_train, y_train)

from sklearn.metrics import classification_report, accuracy_score

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.naive_bayes import MultinomialNB
# from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC,SVC

# models = {
#     "Naive Bayes": MultinomialNB(),
#     "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
#     "SVM": SVC(kernel='linear')
# }

# # Train and evaluate
# results = {}

# for name, model in models.items():
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     acc = accuracy_score(y_test, y_pred)
#     results[name] = acc
#     print(f"\nModel: {name}")
#     print(f"Accuracy: {acc:.2f}")
#     print("Classification Report:")
#     print(classification_report(y_test, y_pred))

def predict_sentiment(text):
    # Preprocess the input text
    cleaned_text = clean_text(text)
    vec=vectorizer.transform([cleaned_text])
    # Make a prediction
    prediction = model.predict(vec)
    return encoder.inverse_transform([prediction[0]])

test=predict_sentiment(f"considering actor i dont think it wast his best work could have done better not that hook.")
print(test[0])

from sklearn.model_selection import GridSearchCV
param_grid = {
    'C': [0.01, 0.1, 1,100],        # Regularization strength
    'penalty': ['l1', 'l2'],             # Regularization penalty
    'solver': ['liblinear', 'saga']      # Solvers that support both penalties
}
logreg=LogisticRegression(max_iter=100)
grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

best_model=grid_search.best_estimator_

import joblib

joblib.dump(best_model, "best_model.pkl")

